01/20/2025
QEA Training - Day 52

ai-testing.md
====================================================================================================
What makes AI software different from traditional software?
1) Learned behavior vs. programmed rules
2) Probabilistic outputs
3) Training data dependency
4) Non-determinism

UNIQUE CHALLENGES OF TESTING AI SYSTEMS
Challenge 1: Non-Deterministic Behavior
The same input might produce slightly different outputs across multiple runs. 
This breaks the fundamental assumption of most testing: "same action, same result."

Challenge 2: The Test Oracle Problem
In traditional testing, the specification tells you what output to expect. 
In AI, defining "correct" is often genuinely difficult.

Challenge 3: Data Dependency
AI models are "data products"—their quality is bounded by the quality of training data.

Challenge 4: Emergent Behavior
AI systems can exhibit behaviors that no one explicitly programmed. 
The model learned patterns from data that produce unexpected results.

THE AI TESTING LIFECYCLE
Testing AI isn't a one-time activity before deployment—it's a continuous process across four phases:
1) Data Testing 
Before training even begins, validate your data:
- Data quality validation (missing values, outliers, format consistency)
- Distribution analysis (is the data balanced? representative?)
- Bias detection in training data
- Data pipeline testing (do transformations work correctly?)
2) Model Testing 
After training, validate the model itself:
- Unit tests for model components
- Accuracy and performance metrics on held-out test data
- Robustness testing (does it handle edge cases?)
- Fairness evaluation (does it perform equally across groups?)
3) Integration Testing 
Validate the model in its deployment context:
- Model + application integration
- API contract testing (correct request/response formats)
- End-to-end flows
- Performance under realistic load
4) Production Testing (Continuous) 
Once deployed, testing never stops:
- Shadow mode testing (run new model alongside old without affecting users)
- A/B testing (compare models on real traffic)
- Monitoring and alerting (detect degradation)
- Continuous evaluation on sampled production data

TESTING STRATEGIES FOR ML MODELS
Strategy 1: Behavioral Testing
Rather than testing internal implementation, test observable behaviors across categories:

Minimum Functionality Tests (MFT): Simple cases the model absolutely must get right.
def test_basic_sentiment():
    """Model must handle clear-cut cases."""
    assert model.predict("I love this!") == "positive"
    assert model.predict("I hate this.") == "negative"
    assert model.predict("It's okay.") == "neutral"
	
Invariance Tests (INV): Changes that shouldn't affect the output.
def test_capitalization_invariance():
    """Capitalizing shouldn't flip sentiment."""
    base = model.predict("Great movie!")
    assert model.predict("GREAT MOVIE!") == base
    assert model.predict("great movie!") == base
	
Directional Expectation Tests (DIR): Changes that should affect output predictably.
def test_negation_effect():
    """Adding 'not' should flip or reduce positive sentiment."""
    positive_score = model.predict("I liked it")
    negative_score = model.predict("I did not like it")
    assert positive_score > negative_score

Strategy 2: Metamorphic Testing
When you can't verify exact outputs, verify relationships between inputs and outputs.

Key insight: Even if you don't know the "correct" translation of a sentence, 
you know that translating English→Spanish→English should produce something similar to the original.
def test_translation_consistency():
    """Round-trip translation should preserve meaning."""
    original = "Hello, how are you?"
    spanish = translate(original, "en", "es")
    back = translate(spanish, "es", "en")
    similarity = compute_similarity(original, back)
    assert similarity > 0.8

Strategy 3: Property-Based Testing
Test that outputs always satisfy certain properties, regardless of specific input.
def test_probabilities_valid():
    """All probability outputs must be valid."""
    for sample in test_samples:
        probs = model.predict_proba(sample)
        # Each probability must be between 0 and 1
        for p in probs:
            assert 0.0 <= p <= 1.0
        # Probabilities must sum to 1
        assert abs(sum(probs) - 1.0) < 0.0001

Test Oracles for AI Systems
When there's no single correct answer, use these oracle strategies:
	Oracle Type		|			Description			|			Example	
Reference Oracle	|	Compare to trusted baseline	|	New model accuracy ≥ current model
Statistical Oracle	|	Check aggregate properties	|	Accuracy on test set ≥ 95%	
Relative Oracle		|	Compare related inputs		|	"Excellent" more positive than "Good"	
Property Oracle		|	Verify invariants			|	Probabilities sum to 1	
Human Oracle		|	Sample for human review		|	Experts review 100 random predictions	


Continuous Testing for AI
Unlike traditional software that works until someone changes the code, AI models can degrade over time even without changes:

Data Drift: The world changes. Customer behavior shifts. New products appear. The patterns the model learned become outdated.

Continuous testing practices:
Regular accuracy checks: Evaluate daily/weekly on fresh labeled samples
Drift detection: Monitor if input distributions are changing
Latency monitoring: Track inference speed over time
Prediction distribution monitoring: Watch for shifts in output patterns
Alerting on degradation: Automated alerts when metrics drop
====================================================================================================


bias-and-fairness.md
====================================================================================================
AI Bias Illustrated:
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│  UNBIASED MODEL:                    BIASED MODEL:                       │
│  ───────────────                    ─────────────                       │
│                                                                         │
│  Applicant Pool:                    Same Applicant Pool:                │
│  ○ ○ ○ ● ● ●                        ○ ○ ○ ● ● ●                         │
│  (○=Group A, ●=Group B)             (Equal qualifications)              │
│                                                                         │
│         │                                   │                           │
│         ▼                                   ▼                           │
│  Approved: ○ ● ○ ●                  Approved: ○ ○ ○ ●                   │
│  (Proportional selection)           (Group B under-selected)            │
│                                                                         │
│  Approval Rate:                     Approval Rate:                      │
│  Group A: 67%                       Group A: 100%                       │
│  Group B: 67%                       Group B: 33%                        │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘

AI bias refers to systematic errors in AI systems that create unfair outcomes, typically disadvantaging certain groups of people. 
The key word is systematic—these aren't random errors. 
Bias is consistent and directional: it reliably produces worse outcomes for specific populations.

Important distinction: Not all model errors are bias. 
If a model incorrectly classifies an image, that's a mistake. 
If the model consistently misclassifies images of one demographic group more often than another, that's bias.

Think of it this way: 
Imagine a loan approval system that approves 70% of male applicants and 50% of female applicants with identical qualifications. 
The 30% and 50% rejection rates might include random errors, but the 20-point gap between groups represents systematic bias.


Bias can enter AI systems at every stage of development—from initial data collection through deployment and beyond. 
Understanding these entry points is essential for testing.
Bias Entry Points:
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│  1. DATA COLLECTION                                                     │
│     └─ Who is included/excluded in training data?                       │
│        Example: Medical AI trained on data primarily from one           │
│        demographic performs poorly on others                            │
│                                                                         │
│  2. DATA LABELING                                                       │
│     └─ How do human labelers' biases affect labels?                     │
│        Example: Labelers rate same resume differently based on          │
│        perceived gender from name                                       │
│                                                                         │
│  3. FEATURE SELECTION                                                   │
│     └─ Which attributes are used for prediction?                        │
│        Example: Using zip code as feature encodes racial segregation    │
│                                                                         │
│  4. ALGORITHM CHOICE                                                    │
│     └─ How does the model optimize, and for whom?                       │
│        Example: Optimizing for overall accuracy may sacrifice           │
│        minority group performance                                       │
│                                                                         │
│  5. DEPLOYMENT CONTEXT                                                  │
│     └─ How is the model used in practice?                               │
│        Example: Model designed for one context applied to another       │
│                                                                         │
│  6. FEEDBACK LOOPS                                                      │
│     └─ How do model predictions affect future data?                     │
│        Example: Predictive policing creates more arrests in             │
│        already-policed areas, reinforcing patterns                      │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
1. Data Collection Bias
The problem: Training data doesn't represent the full population the model will serve.
Example: A voice assistant trained primarily on native English speakers performs poorly for users with accents. 
A dermatology AI trained mostly on images of lighter skin struggles to diagnose conditions on darker skin.
Testing implication: Check whether your test data represents all user groups, not just the majority.

2. Data Labeling Bias
The problem: Human labelers inject their own biases into training labels.
Example: When human annotators rate resume quality, 
they might unconsciously rate identical resumes differently based on whether the name sounds male or female, 
or based on ethnic associations with names.
Testing implication: Audit label distributions across demographic groups. 
If possible, use multiple labelers and measure inter-annotator agreement.

3. Feature Selection Bias
The problem: The attributes chosen as model inputs encode biased information, 
even if protected attributes aren't explicitly included.
Example: Using zip code as a feature for loan approval seems neutral, 
but it can serve as a proxy for race due to historical residential segregation. 
Using "attended a historically Black college" correlates directly with race.
Testing implication: Review features for potential proxy effects. 
Test whether removing proxies changes model behavior across groups.

4. Algorithm Bias
The problem: The optimization objective prioritizes overall accuracy, 
which may sacrifice performance for minority groups.
Example: A model optimized for overall accuracy might achieve 95% accuracy 
by getting 98% accuracy on the majority group (90% of data) 
and 70% accuracy on the minority group (10% of data). 
The overall metric looks good, but the minority group experience is poor.
Testing implication: Always evaluate metrics separately by demographic group, not just overall.

5. Deployment Context Bias
The problem: A model designed for one context is deployed in a different context where its assumptions don't hold.
Example: A facial recognition system trained and tested on US demographics is deployed 
in a country with different demographic distributions, where it performs worse.
Testing implication: Test with data representative of actual deployment context, not just training context.

6. Feedback Loop Bias
The problem: Model predictions influence future data collection, amplifying existing biases.
Example: Predictive policing sends more officers to areas the model flags as high-crime. 
More officers make more arrests, generating more crime data from those areas. 
This data reinforces the model's belief those areas are high-crime—regardless of actual crime rates elsewhere.
Testing implication: Monitor for amplification effects over time. Track whether disparities are increasing.


TYPES OF BIAS
Selection Bias
The training data systematically excludes or underrepresents certain populations.
Real-world example: Medical AI trained primarily on data from academic medical centers may perform poorly 
in rural or community health settings with different patient populations.
How to test: Compare demographic distribution in training data to target population distribution.

Confirmation Bias
Looking for patterns that confirm existing beliefs or practices.
Real-world example: If fraud investigators have historically focused on certain countries, 
more labeled fraud examples exist from those countries. 
A model trained on this data will flag transactions from those countries more often—not because 
fraud is actually higher, but because that's where investigators looked.
How to test: Audit label distributions by demographic. 
Question whether patterns in data reflect reality or historical practices.

Measurement Bias
Different accuracy in measuring attributes across groups.
Real-world example: Pulse oximeters are less accurate on darker skin tones. 
AI health monitors trained on this data inherit inaccurate readings for Black patients, 
potentially underestimating how sick they are.
How to test: Validate that measurement accuracy is consistent across groups for input features.

Aggregation Bias
A single model applied to diverse subgroups that shouldn't be treated uniformly.
Real-world example: A diabetes risk model that works well on average 
but fails for specific ethnic groups whose risk factors differ from the majority population.
How to test: Evaluate model performance separately on each subgroup rather than only in aggregate.

Historical Bias
Data reflects past discrimination that we shouldn't perpetuate.
Real-world example: Hiring AI trained on historical hiring decisions learns to 
prefer candidates similar to those historically hired—
even if those patterns reflected past discrimination rather than actual job qualifications.
How to test: Question whether historical patterns should be replicated. Ask: 
"Is this pattern what should happen, or just what did happen?"


FAIRNESS CONCEPTS
Fairness means different things to different stakeholders.
Several mathematical definitions exist, and importantly, 
they're often mutually exclusive—you can't satisfy all of them simultaneously.

Protected attributes are characteristics for which discrimination is legally prohibited or ethically concerning.
Critical concept: Proxy variables. 
Even if you remove protected attributes from model inputs, 
biased outcomes can persist through proxy variables—features that correlate with protected attributes.

Common Protected Attributes:
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│  LEGALLY PROTECTED (varies by jurisdiction):                            │
│  • Race / Ethnicity                                                     │
│  • Gender / Sex                                                         │
│  • Age                                                                  │
│  • Religion                                                             │
│  • National Origin                                                      │
│  • Disability Status                                                    │
│  • Genetic Information                                                  │
│                                                                         │
│  OFTEN ETHICALLY CONCERNING:                                            │
│  • Socioeconomic Status                                                 │
│  • Sexual Orientation                                                   │
│  • Geographic Location (as proxy)                                       │
│  • Education Level (as proxy)                                           │
│                                                                         │
│  PROXY VARIABLES (encode protected attributes indirectly):              │
│  • Zip Code → Race/Income                                               │
│  • Name → Gender/Ethnicity                                              │
│  • College Attended → Socioeconomic Status                              │
│  • Club Memberships → Gender                                            │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘


Disparate impact occurs when a seemingly neutral practice disproportionately affects a protected group, regardless of intent.
This is a legal concept with practical testing implications.

The Four-Fifths (80%) Rule: A commonly used threshold for identifying potential disparate impact in employment contexts. 
If the selection rate for a protected group is less than 80% (four-fifths) of the rate for the most-favored group, 
disparate impact may exist.

MATHEMATICAL DEFINITIONS OF FAIRNESS
Different stakeholders have different ideas about what "fairness" means. 
Several mathematical definitions exist, and importantly, they're often mutually exclusive.

Demographic Parity
Equal selection rates across groups, regardless of qualifications.
"The percentage of applicants approved should be the same for all demographic groups."
When to use: When historical qualifications may themselves reflect past discrimination.
Limitation: Ignores actual qualifications. May approve unqualified people from one group while rejecting qualified people from another.

Equalized Odds
Equal true positive rates AND equal false positive rates across groups.
"The model should be equally accurate (same error rates) for all groups."
When to use: When you have reliable ground truth labels and want the model to treat similar individuals similarly.
Limitation: Requires accurate ground truth labels, which may themselves be biased.

Predictive Parity
Equal precision across groups.
"When the model says 'yes,' it should be equally likely to be correct for all groups."
When to use: When the consequences of positive predictions need to be justified equally across groups.
Limitation: Can coexist with very different treatment of the groups.

THE IMPOSSIBILITY THEOREM
Critical insight: Except in very specific cases, 
it's mathematically impossible to satisfy demographic parity, 
equalized odds, and predictive parity simultaneously.

Why this matters: Teams must choose which definition of fairness matters most for their context. 
There is no "perfectly fair" system that satisfies all definitions at once.

ETHICAL IMPLICATIONS
Bias in AI isn't just a technical problem—it has profound ethical dimensions. 
When testing for bias, consider these questions:

Harm assessment:
Who is harmed by the model's errors?
What is the severity of harm? (Minor inconvenience vs. denied opportunities vs. physical harm)
Are harms distributed equitably across groups?
Can individuals challenge or appeal decisions?

Power dynamics:
Who benefits from the system?
Who bears the costs of errors?
Do affected people have a voice in system design?

Alternatives:
Is AI the right solution for this problem?
What happens without AI involvement? (Is the status quo actually better?)
Are there lower-stakes alternatives?

REGULATORY REQUIREMENTS
The regulatory landscape for AI fairness is evolving rapidly:
EU AI Act (2024)
- High-risk AI systems must undergo conformity assessments
- Mandatory bias testing for systems affecting fundamental rights
- Human oversight requirements
- Documentation and logging obligations

US Regulations (varies by sector)
- Equal Credit Opportunity Act (credit decisions)
- Fair Housing Act (housing)
- Title VII of Civil Rights Act (employment)
- Americans with Disabilities Act
- NYC Local Law 144 (automated employment decision tools)

Industry standards:
- IEEE 7010 (Wellbeing Impact Assessment)
- ISO/IEC TR 24028 (AI Trustworthiness)
- NIST AI Risk Management Framework

BIAS IN TESTING TOOLS
Even your testing infrastructure can contain bias:

Test data bias: Benchmark datasets may not represent all populations. 
"Standard" test scenarios often default to majority group characteristics.

Tooling bias: Synthetic data generators embed assumptions. 
Performance testing optimizes for average-case, missing worst-case experiences for minorities.

Process bias: QA teams lacking diversity may miss issues. 
Bug triage may deprioritize issues affecting smaller user groups.

Mitigation:
- Include demographic analysis in test plans
- Use diverse test personas
- Explicitly test for differential performance across groups
- Include fairness in your definition of "done"
====================================================================================================

# If we're snowed in on Friday (01/23/2026), we'll do the presentations fully remote.